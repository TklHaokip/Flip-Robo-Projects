			WORKSHEET
		DEEP LEARNING – WORKSHEET 5


Q1 to Q8 are MCQs with only one correct answer. Choose the correct option.
1. Which of the following are advantages of batch normalization?
A) Reduces internal covariant shift.
B) Regularizes the model and reduces the need for dropout, photometric distortions, local response
normalization and other regularization techniques.
C) allows use of saturating nonlinearities and higher learning rates.
D) All of the above
Answer:- D) All of the above





2. Which of the following is not a problem with sigmoid activation function?
A) Sigmoids do not saturate and hence have faster convergence
B) Sigmoids have slow convergence.
C) Sigmoids saturate and kill gradients.
D) Sigmoids are not zero centered; gradient updates go too far in different directions, making optimization
more difficult.
Answer:- A) Sigmoids do not saturate and hence have faster convergence





3. Which of the following is not an activation function?
A) Swish B) Maxout
C) SoftPlus D) None of the above
Answer:- A) Swish




4. The tanh activation usually works better than sigmoid activation function for hidden units because the mean of
its output is closer to zero, and so it centers the data better for the next layer. True/False?
A) True B) False
Answer:- A) True





5. In which of the weights initialisation techniques, does the variance remains same with each passing layer?
A) Bias initialisation B) Xavier Initialisation
C) He Normal Initialisation D) None of these
Answer:- B) Xavier Initialisation





6. Which of the following is main weakness of AdaGrad?
A) learning rate shrinks and becomes infinitesimally small
B) learning rate doesn’t shrink beyond a point
C) change in learning rate is not adaptive
D) AdaGrad adapts updates to each individual parameter
Answer:- A) learning rate shrinks and becomes infinitesimally small





7. In order to achieve right convergence faster, which of the following criteria is most suitable?
A) momentum and learning rate both must be high
B) momentum must be high and learning rate must be low
C) momentum and learning rate both must be low
D) momentum must be low and learning rate must be high
Answer:- D) momentum must be low and learning rate must be high





8. When is an error landscape is said to be poor(ill) conditioned?
A) when it has many local minima
B) when it has many local maxima
C) when it has many saddle points and flat areas
D) None of these
Answer:- C) when it has many saddle points and flat areas





Q9 and Q10 are MCQs with one or more correct answers. Choose all the correct options.
9. Which of the following Gradient Descent algorithms are adaptive?
A) ADAM B) SGD
C) NADAM D) RMS Prop.
Answer:- A) ADAM
	 D) RMS Prop.





10. When should an optimization function (gradient descent algorithm) stop training:
A) when it reaches local minimum B) when it reaches saddle point
C) when it reaches global minimum
D) when it reaches a local minima which is similar to global minima (i.e. which has very less error distance
with global minima) 
Answer:- D) when it reaches a local minima which is similar to global minima (i.e. which has very less error distance
with global minima) 




Q11 to Q15 are subjective answer type question. Answer them briefly.
11. What are convex, non-convex optimization?
Answer:- A convex optimization problem is a problem where all of the constraints are convex functions, and the objective is a convex function if minimizing, or a concave function if maximizing. ... A non-convex optimization problem is any problem where the objective or any of the constraints are non-convex.




12. What do you mean by saddle point? Answer briefly.
Answer:- a point at which a function of two variables has partial derivatives equal to zero but at which the function has neither a maximum nor a minimum value.





13. What is the main difference between classical momentum and Nesterov momentum? Explain briefly.
Answer:- Momentum and Nesterov Momentum (also called Nesterov Accelerated Gradient/NAG) are slight variations of normal gradient descent that can speed up training and improve convergence significantly.






14. What is Pre initialisation of weights? Explain briefly.
Answer:- The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. ... Matrix multiplication is the essential math operation of a neural network.





15. What is internal covariance shift in Neural Networks?
Answer:- An internal covariate shift occurs when there is a change in the input distribution to our network. When the input distribution changes, hidden layers try to learn to adapt to the new distribution. This slows down the training process.
	 In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on.








